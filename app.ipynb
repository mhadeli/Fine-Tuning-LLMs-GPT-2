{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1. Loading the dataset\n",
    "The code loads a dataset called \"mteb/tweet_sentiment_extraction\" using the load_dataset function from the datasets library. This dataset contains tweets with sentiment labels (positive, negative, or neutral)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "df = pd.DataFrame(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  label  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going      1   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2  088c60f138                          my boss is bullying me...      0   \n",
       "3  9642c003ef                     what interview! leave me alone      0   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "\n",
       "  label_text  \n",
       "0    neutral  \n",
       "1   negative  \n",
       "2   negative  \n",
       "3   negative  \n",
       "4   negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Tokenizing the dataset\n",
    "The code then tokenizes the dataset using the GPT2Tokenizer from the transformers library. Tokenization is the process of breaking down text into smaller units, such as words or subwords, that can be fed into a neural network. The tokenizer is trained on the \"gpt2\" model and is configured to pad the input text to a maximum length and truncate it if it's too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Loading the dataset to train our model\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Creating a small training and evaluation dataset\n",
    "The code creates a small training dataset (small_train_dataset) and a small evaluation dataset (small_eval_dataset) by shuffling and selecting a subset of the original dataset. This is done to speed up training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Defining the model and training arguments\n",
    "The code defines a GPT2ForSequenceClassification model, which is a variant of the GPT-2 model that's specifically designed for sequence classification tasks (i.e., predicting a label for a sequence of text). The model is initialized with the \"gpt2\" pre-trained weights and is configured to have 3 output labels (positive, negative, and neutral).\n",
    "The code also defines a set of training arguments using the TrainingArguments class from the transformers library. These arguments control various aspects of the training process, such as the output directory, batch size, and gradient accumulation steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Evaluate the Fine-tuned Model\n",
    "Evaluate the fine-tuned model on the validation set to assess its performance. You can use metrics such as accuracy, precision, recall, and F1-score to evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 6. Use the Fine-tuned Model\n",
    "Use the fine-tuned model. We can use the model to generate text, classify text, or perform other tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe112f990d044eea1efeef3a4b18eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.017, 'grad_norm': 14.172191619873047, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.0}\n",
      "{'train_runtime': 2255.123, 'train_samples_per_second': 1.33, 'train_steps_per_second': 0.333, 'train_loss': 0.8688200073242187, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=750, training_loss=0.8688200073242187, metrics={'train_runtime': 2255.123, 'train_samples_per_second': 1.33, 'train_steps_per_second': 0.333, 'total_flos': 1567794659328000.0, 'train_loss': 0.8688200073242187, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "   output_dir=\"test_trainer\",\n",
    "   #evaluation_strategy=\"epoch\",\n",
    "   per_device_train_batch_size=1,  # Reduce batch size here\n",
    "   per_device_eval_batch_size=1,    # Optionally, reduce for evaluation as well\n",
    "   gradient_accumulation_steps=4\n",
    "   )\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=small_train_dataset,\n",
    "   eval_dataset=small_eval_dataset,\n",
    "   compute_metrics=compute_metrics,\n",
    "\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 7. Evaluate the model's performance on a validation or test set. \n",
    "The trainer class already contains an evaluate method that takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1757895946502686,\n",
       " 'eval_accuracy': 0.537,\n",
       " 'eval_runtime': 224.3747,\n",
       " 'eval_samples_per_second': 4.457,\n",
       " 'eval_steps_per_second': 4.457,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refrence\n",
    "\n",
    "Datacamp\n",
    "https://www.datacamp.com/tutorial/fine-tuning-large-language-models\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
